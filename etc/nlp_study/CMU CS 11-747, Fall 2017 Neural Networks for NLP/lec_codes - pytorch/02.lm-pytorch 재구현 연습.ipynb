{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to read in the corpus\n",
    "# NOTE: We are using data from the Penn Treebank, which is already converted\n",
    "#       into an easy-to-use format with \"<unk>\" symbols. If we were using other\n",
    "#       data we would have to do pre-processing and consider how to choose\n",
    "#       unknown words, etc.\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "S = w2i[\"<s>\"]\n",
    "UNK = w2i[\"<unk>\"]\n",
    "def read_dataset(filename):\n",
    "  with open(filename, \"r\") as f:\n",
    "    for line in f:\n",
    "      yield [w2i[x] for x in line.strip().split(\" \")]\n",
    "\n",
    "# Read in the data\n",
    "train = list(read_dataset(\"../data/ptb/train.txt\"))\n",
    "w2i2 = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/ptb/valid.txt\"))\n",
    "i2w = {v: k for k, v in w2i.items()}\n",
    "n_words = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-forward Neural Network Language Model\n",
    "\n",
    "class FFNN_LM(nn.Module):\n",
    "    def __init__(self, n_words, embed_size, hidden_size, n_gram, dropout_rate = 0.5):\n",
    "        super(FFNN_LM, self).__init__()\n",
    "        self.embedding = nn.Embedding(n_words, embed_size)\n",
    "        self.fnn = nn.Sequential(\n",
    "            nn.Linear(n_gram * embed_size, hidden_size), nn.Tanh(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, n_words)\n",
    "        )\n",
    "    \n",
    "    def forward(self, word_idxs):\n",
    "        embed = self.embedding(word_idxs)\n",
    "        embed_reshape = embed.view(embed.size(0), -1)\n",
    "        logits = self.fnn(embed_reshape)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2\n",
    "EMB_SIZE = 128\n",
    "HID_SIZE = 128\n",
    "DROPOUT_RATE = 0.5\n",
    "\n",
    "model = FFNN_LM(n_words=n_words, embed_size=EMB_SIZE, hidden_size= HID_SIZE, n_gram=N, dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_idxs_to_variable(word_idxs):\n",
    "    var = Variable(torch.LongTensor(word_idxs))\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_logits_for_sents(sents):\n",
    "    var = word_idxs_to_variable(sents)\n",
    "    logits = model(var)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(sents):\n",
    "    n_gram_sent = [S] * N\n",
    "    n_gram_history = []\n",
    "    target_word_history = []\n",
    "    \n",
    "    for sent in sents:\n",
    "        for target_word in sent + [S]:\n",
    "            \n",
    "            # 앞에 등장하는 N개의 단어와 target_word를 매치 시킨다.\n",
    "            n_gram_sent = n_gram_sent[1:] + [target_word]\n",
    "            \n",
    "            # 학습을 위해, feature(previous N-words)와 label(target_word)을 저장\n",
    "            n_gram_history.append([n_gram_sent])\n",
    "            target_word_history.append(target_word)\n",
    "    \n",
    "    logits = cal_logits_for_sents(n_gram_history)\n",
    "    target = word_idxs_to_variable(target_word_history)\n",
    "    loss = F.cross_entropy(input=logits, target=target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data):\n",
    "    lr = 0.0001\n",
    "    epochs = 5\n",
    "    \n",
    "    train_sents, train_cost = 0\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        for i, data in train_data:\n",
    "            loss = cal_loss(data)\n",
    "            train_cost += loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = cal_loss([train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_scores_for_sents(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.randn(1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5469,  1.4084,  2.8021, -0.6644,  1.1593]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(test).multinomial(1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.opti,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
