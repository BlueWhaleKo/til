# Predicting the next word in a sentence
> 한 문장에서, 다음에 오는 단어를 어떻게 예측할까?

> 잘못된 단어가 오면 loss를 크게 하고, 적절한 단어가 오면 loss를 작게 하는 방식으로 업데이트(학습) 시킨다면 가능하지 않을까?

#

### 어떻게 해야 할까?

#

![](assets/markdown-img-paste-2018121518501981.png)


### ex) "나는 영화관에서 영화를 __ " 과 같은 문장(n-gram)이 주어졌다고 하자.

> 앞에서 "나는 영화관에서 영화를" 이라는 n-1개의 단어 조합이 등장했을 때, 다음에 등장할(n번째) 확률이 가장 높은 단어(봤다)를 선택하면 되지 않을까?

#### 어떤 단어가 가장 등장할 확률이 높을까?

#### 위 질문에 대답하기 위해 language model에 대해 짚고 넘어가자.

## 1. Count-based Language Models
> 빈도수 기반의 language model   

![](assets/markdown-img-paste-20181215202112635.png)
> 내가 만약 100만개의 문장(데이터)을 가지고 있다고 가정해보자.

> 내가 가진 데이터에서, "나는 영화관에서 영화를 " 이라는 문장이 100번 등장했는데, 그 뒤에 등장한 단어들을 살펴보니,
>각각 '봤다'가 80번, '본다'가 19번, '먹다'가 1번 등장했다.

> #### 추론 : 내가 가진 데이터에서 추론해봤을 때, "내가 영화관에서 영화를" 다음에는 '봤다'라는 단어가 오는게 가장 적절할 것 같아!

#

### 잘 추론했는지 평가하기 위한 지표로 무엇을 사용할까?

![](assets/markdown-img-paste-20181222095334480.png)

## Q. 크로스 엔트로피에서 밑이 2인 로그를 사용하는 이유는?
> 섀넌 엔트로피 : 컴퓨팅에서 밑이 2인 로그를 사용하면 연산량(bite)를 계산할 수 있음.

#

n-gram 모델은 굉장히 좋은 성능을 보여주었고, 오랜 기간 동안 language model의 base-line의 자리를 지키고 있었다.

하지만 완벽한 모델은 아님.

### N-gram 모델의 문제는?
#### 1. 유사단어 등장의 문제
> "나는 영화관에서 영화를 봤다"와 "나는 영화관에서 영화를 본다"는 사실 거의 동일한 문장.
> 하지만, 컴퓨터는 '봤다'와 '본다'가 유사한 의미를 갖는 단어라는걸 모른다. 그냥 단순하게, 등장한 횟수만 계산할 뿐이다.

##### solution : class based language model

#

#### 2. 중간에(intervening) 오는 단어는 추측을 못한다.
![](assets/markdown-img-paste-20181222102111513.png)
> Dr. OOOO Smith이라는 단어는 꽤 빈번하게 등장하는 n-gram이지만, 중간에 오는 단어의 학률은 계산하지 못한다.

##### solution : skip-gram language model (Word2Vec)

#

#### 3. 멀리 떨어져 있는 단어의 관계를 추론하지 못한다.
##### 나는 **프로그래밍** 수업을 위해서 이번에 새로운 ____ "
##### "**테니스** 동아리에 들어간 기념으로, 이번에 새로운  ____ "

> 우리는 첫 번째 문장에는 '**노트북**' 이나 '**컴퓨터**'라는 단어가 등장할 것이라고 유추할 수 있을 것이다.
> 두번째 문장에서는 '**라켓**'이라는 단어가 등장할 것이다.

> 우리가 이렇게 추론할 수 있는 이유는, **'프로그래밍'과 '컴퓨터'**, **'테니스' 와'라켓'** 의 조합이 문맥상에서 주로 함께 등장하는 단어임을 알고 있기 때문이다.

##### soultuin : cache, trigger, topic, syntactic models

#
#

## Altenative : Featurized Log-Linear Models
> 문맥의 features를 계산한다
> feature에 기반하여 확률을 계산한다.
> 경사면하강(Gradient Descent) 알고리즘들을 활용하여, feature의 가중치(weight)를 최적화 한다.

#
#

### Example : Linear neg-log 모델
#### 다음에 올 수 있는 모든 단어들에 대해서 점수를 계산한다.

![](assets/markdown-img-paste-20181222110200911.png)

#### 계산된 점수에 softmax함수를 적용시켜 확률로 만들어준다.

![](assets/markdown-img-paste-20181222110326762.png)

##### softmax함수란?
> 쉽게 생각하면, **수의 집합이 주어졌을 때**, 각각의 점수에 exponential을 취해주고, 전체의 합으로 나눠줌으로써(Normalize) **확률의 형태로 바꿔주는 함수** 이다.

> **출력층에서 제일 큰 값을 갖는 단어가, 가장 큰 확률을 갖는 방식** 이다.
> 가중치(weight)를 업데이트(train) 할 때도, 가장 그럴듯한 단어의 출력값이 가장 큰 값을 갖도록 업데이트가 된다.

> 위의 예시에서는 **giving a** 다음에는 **gift** 라는 단어가 등장할 확률(44.4%)이 가장 높다!!

#

### 간단한 예시

![](assets/markdown-img-paste-20181222111237927.png)

#

### 매번 등장하는 'Lookup'이 도대체 무슨 의미일까?

![](assets/markdown-img-paste-20181222111959414.png)

> a big matrix of word embeddings(개별 단어가 벡터의 형태로 표현된 행렬)에서 특정 단어의 벡터를 가져오는 것.

> 미리 단어들의 벡터(수치)를 정의해두고, 해당 단어가 등장할 때마다, 앞서 만들어둔 table(행렬 : matrix)에서 해당 단어의 벡터(수치)를 가져오는 방법론.

영어 문장을 한글로 번역한다고 할 때, **영단어 사전(embedding matrix)에서 영단어의 뜻(word vector)를 찾는 것으로 비유하면 비슷할 것 같다!**

#
#

## Model 학습
> ML/DL에서 **학습(training)이란**, 사용자가 정의한 loss function(비용 함수: 모델의 오차)을 최소화(minimize) 방식으로 parameter(weight)를 업데이트 하는 것을 의미한다.

> **일반적으로 loss function은 음의 로그우도(negative log likelihood)가 가장 많이 사용된다.**

### negative log likelihood를 쓰는 이유는?
> **log likelihood를 maximize** 하는 방식으로 파라미터를 업데이트 하게 되면, **KL-Divergence(cross-entropy)를 최소화 하는 문제와 본질적으로 동일** 하게 된다.

> **KL-Divergence(cross-entropy)는 data의 분포와 학습시키는 model의 분포 사이의 차이** 라고 볼 수 있다. 즉, model을 우리가 가진 데이터의 실제 분포와 동일하게 만들어가는 작업

#

## Parameter update
> **오류역전파(back-propagation)** 방식으로, loss function(비용 함수)에 대해, 각각의 parameter의 derivative(편미분한 값, 1계 도함수)를 계산하여, 경사면하강(Gradient Descent) 알고리즘으로 parameter를 업데이트 한다.

![](assets/markdown-img-paste-20181222221601237.png)

#

## Evaluation and Vocabulary
> **※중요** 평가하는 모델에 학습된 단어들이 모두 동일해야 한다. 정확히 말하면, 모든 모델이 test set를 생성(generate)할 수 있어야 한다.

#
#

## N-gram의 문제가 해결되었을까?
#### 1. 유사단어 문제
> 아직 해결하지 못함.. ㅠㅠ

#### 2. 중간에 등장하는 단어
> 해결됨!

#### 3. 멀리 떨어진 단어의 관계
> 아직 해결하지 못함.. ㅠㅠ

#

## 선형(linear) 모델은 Feature Combination을 학습하지 못한다.
> xor 문제를 풀 수 없음!

![](assets/markdown-img-paste-20181222223834442.png)

> 모든 feature combination의 vector를 모두 기억할 수 없다.
#
#

## 더 나아가 보자 (non-linear model : Neural language models)

![](assets/markdown-img-paste-20181223101544381.png)

> lookup은 embedding table을 사용한다.
> **embedding** 은 두 단어가 유사하면, 두 단어는 각각 유사한 n차원 벡터로 표현되는 방법론.

> **tanh** : input의 features를 결합하여 하나의 노드 or 소수의 노드로 변형된 출력값(output)을 준다. (비선형성(non-linearlity)을 얻는다)

#

## 어떻게 하면 개선할 수 있을까?

![](assets/markdown-img-paste-20181224230153661.png)

> 1. pre-train된 embedding을 사용한다.
> 2. softmax를 적용하기 전 logit의 weight-matrix(n_words x embedding_size)의 행(row)을 input(단어)으로 활용한다. 결과적으로 input과 output이 유사해지므로(approximatly same(?)), 결과가 개선된다.

#

# 학습 Tricks

## 1. 학습 마다 데이터의 순서를 섞는다 (Shuffling)
> Stochastic gradient (SGD) 알고리즘은, 학습 데이터를 하나씩 (혹은 작은 그룹 :mini-batch) 입력받아서 파라미터를 세부 조정한다. 따라서, 입력받는 순서가 고정될 경우, 결과가 편향될 수 있다.

> ex) 만약, 매 학습(epoch) 마다, 처음과 마지막에 학습한 문장이 '나는 으아아아악' 이라고 하자. 위 경우, 해당 모델은 '나는' 이라는 단어 다음에는 '으아아아악'이 나온다는 것에 높은 가중치를 두는 모델이 만들어질 수도 있다.

#

## 2. 최적화 알고리즘 선택
> batch-GD, SGD, Momentum, Adadelta, Adagrad, Adam

> batch-GD : 가장 기본적인 경사면하강 알고리즘, learning-rate가 상수로 고정되어 있으며, 전체 데이터에 대해 도함수(gradient)를 한번에 계산하여서 업데이트 하는 방식

> SGD : batch-GD와 달리, 학습 데이터를 하나씩 혹은 조금씩(mini-batch) 나눠서 학습하는 알고리즘. batch 방식 대비, 속도가 빠르지만, 데이터를 하나씩 학습하다보니 cost가 수렴하는 과정에서 진동이 심함.

#

> Momentum : SGD를 개선한 알고리즘. 과거에 업데이트한 파리미터를 저장하여서, 해당 방향으로 학습을 조정(adapt)하는 최적화 알고리즘. learning-rate는 상수이며, update-history를 (1-γ)만큼 decay시킨다.
![](assets/markdown-img-paste-20181224232133628.png)

#

> Adagrad : Adaptive Gradient 알고리즘. 개별 파라미터에 대해 학습 속도를 조정하는 것이 핵심. 과거의 업데이트를 기록하여서, 과거에 많이 학습이 이루어진 파라미터의 learning-rate를 상대적으로 큰 수(과거 업데이트의 제곱합) 나눠주어, 학습 속도를 늦추는 방식
![](assets/markdown-img-paste-20181224232627992.png)
> 하지만, 모든 과거 업데이트를 동일한 가중치(제곱합)으로 기록하다보니, learning-rate가 너무 빠르게 decay되는 문제가 발생 -> 이를 개선하기 위해, Adadelta와 Adam 알고리즘이 등장

#

> Adam : Adagrad와 Momentum을 합친 형태의 알고리즘. 과거 학습이 많이 진행된 파라미터의 학습 속도를 줄임(Adagrad)과 동시에, 과거의 학습 경로를 기억하여서 해당 방향으로 학습이 진행되도록 조정(Momentum)하는 방식의 알고리즘.
![](assets/markdown-img-paste-2018122423305307.png)

> 보다 자세한 설명 혹은 구체적인 수식이 궁금하신 분들께서는 아래의 링크를 참조해주세요.

http://ruder.io/optimizing-gradient-descent/index.html#whichoptimizertochoose

### 어떤 알고리즘을 사용해야 할까?
> 1. **Adam** 이 일반적으로 **수렴이 빠르고 성능도 우수하다**.
> 2. SGD가 학습이 오래 걸리긴 하지만, 최종적으로는 성능이 매우 뛰어난 편 (in terms of generalization)
> 3. **learning-rate decay를 꼭 사용하는 것이 좋다.** (learning-rate를 조정하는 최적화 알고리즘(ex: Adam, Adadelta)등을 활용하자

#

## 3. 학습 중단(Early Stopping)
> 학습 중, cost가 수렴하지 않고 발산하는 경우, 중간에 학습을 중단한다.
> train cost와 test cost를 꾸준히 비교하면서, 과적합(over-fitting) 되고 있는지 꼭 확인해야 할 필요가 있다.

#

## 4. Dropout
> 학습 시에, hidden layer에서 임의의 노드를 끄는 방식(drop)으로, 모델이 학습 데이터에 과적합되는 것을 방지한다.
> 과적합(over-fittin)을 피하는 가장 좋은 방법론 중 하나.

#

## 5. Mini-batching
> 최근 사용되는 하드웨어에서는, size 1의 10개 연산보다 size 10의 1개 연산이 훨씬 빠르다.
> 유사한 연산들을 한번에 합쳐서 하나의 큰 연산으로 만들면 훨씬 효율적이다.

![](assets/markdown-img-paste-2018122500133042.png)
