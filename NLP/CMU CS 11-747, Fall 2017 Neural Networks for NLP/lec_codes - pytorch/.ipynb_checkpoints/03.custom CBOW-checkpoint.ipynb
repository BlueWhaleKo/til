{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to read in the corpus\n",
    "# NOTE: We are using data from the Penn Treebank, which is already converted\n",
    "#       into an easy-to-use format with \"<unk>\" symbols. If we were using other\n",
    "#       data we would have to do pre-processing and consider how to choose\n",
    "#       unknown words, etc.\n",
    "w2i = defaultdict(lambda: len(w2i))\n",
    "S = w2i[\"<s>\"]\n",
    "UNK = w2i[\"<unk>\"]\n",
    "def read_dataset(filename):\n",
    "  with open(filename, \"r\") as f:\n",
    "    for line in f:\n",
    "      yield [w2i[x] for x in line.strip().split(\" \")]\n",
    "\n",
    "# Read in the data\n",
    "train = list(read_dataset(\"../data/ptb/train.txt\"))\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/ptb/valid.txt\"))\n",
    "i2w = {v: k for k, v in w2i.items()}\n",
    "n_words = len(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_words, embed_size, hidden_size, dropout_rate, window_size):\n",
    "        super(CBOW,self).__init__()\n",
    "        self.embedding = nn.Embedding(n_words, embed_size)\n",
    "        \n",
    "        self.cbow = nn.Sequential(\n",
    "            nn.Linear(embed_size, hidden_size), nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, n_words),\n",
    "        )\n",
    "        return\n",
    "    \n",
    "    def _convert_word_idx_to_variable(self, word_idxs):\n",
    "        var = Variable(torch.LongTensor(word_idxs))\n",
    "        return var\n",
    "\n",
    "    def forward(self, word_idxs):\n",
    "        if not type(word_idxs) == torch.Tensor:\n",
    "            word_idxs = self._convert_word_idx_to_variable(word_idxs)\n",
    "        \n",
    "        embed = self.embedding(word_idxs) #[N, context_size(2*window_size), embed_size]\n",
    "        embed = embed.sum(1) # [N, embed_size]\n",
    "        logit = self.cbow(embed)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "hidden_size = 128\n",
    "dropout_rate = 0.3\n",
    "window_size = 2\n",
    "\n",
    "model = CBOW(\n",
    "    n_words=n_words, \n",
    "    embed_size=embed_size,\n",
    "    hidden_size=hidden_size, \n",
    "    dropout_rate=dropout_rate, \n",
    "    window_size=window_size,\n",
    "    )\n",
    "\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_word_idx_to_variable(word_idxs):\n",
    "    var = Variable(torch.LongTensor(word_idxs))\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_logits(sents):\n",
    "    \n",
    "    if not type(sents) == torch.Tensor:\n",
    "        sents = convert_word_idx_to_variable(sents)\n",
    "        \n",
    "    logit = model(sents)\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(sents):\n",
    "    context_history = []\n",
    "    target_word_history = []\n",
    "    \n",
    "    for sent in sents:\n",
    "        for i, target_word in enumerate(sent):\n",
    "            context = [S] * window_size + [target_word] + [S] * window_size\n",
    "            \n",
    "            for j, w in enumerate(range(-window_size, window_size+1)):\n",
    "                if i+w <0  or i+w > len(sent)-1:\n",
    "                    pass\n",
    "                else:\n",
    "                    context[j] = sent[i+w] # 중심단어 기준에서 w개 전(후) 단어\n",
    "                    \n",
    "                    # 앞뒤 N개 단어를 context_hitory에\n",
    "                    # 중간의 target 단어를 target_word_history에 저장\n",
    "            context_history.append([context[0:window_size] + context[window_size+1:]])\n",
    "            target_word_history.append(target_word)\n",
    "\n",
    "            context_var = convert_word_idx_to_variable(context_history)\n",
    "            context_var = context_var.view(context_var.size(0), -1)\n",
    "            target_var = convert_word_idx_to_variable(target_word_history)\n",
    "\n",
    "    logits = cal_logits(context_var)\n",
    "    cost = F.cross_entropy(logits, target_var, reduction = 'sum')\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 100 # mini-batch GD\n",
    "\n",
    "# 모델을 학습모드로 변경\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    random.shuffle(train)\n",
    "    train_loss = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for start_idx, end_idx in zip(range(0, len(train), batch_size),\n",
    "                                  range(batch_size, len(train)+1, batch_size)):\n",
    "        sents = train[start_idx : end_idx]\n",
    "        my_loss = cal_loss(sents)\n",
    "        train_loss += my_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        my_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch+1)%10 == 0 :\n",
    "        print('%s epoch 학습 완료. 경과 시간 : %s'%(epoch+1, start-time - time.time()))\n",
    "        print('Train Loss : %s'%train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
