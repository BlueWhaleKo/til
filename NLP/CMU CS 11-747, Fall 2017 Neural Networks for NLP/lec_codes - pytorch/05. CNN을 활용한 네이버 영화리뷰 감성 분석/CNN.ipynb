{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data는 e9t(Lucy Park)님께서 github에 공유해주신 네이버 영화평점 데이터를 사용하였습니다.\n",
    "# https://github.com/e9t/nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i_dict = defaultdict(lambda : len(w2i_dict))\n",
    "pad = w2i_dict['<PAD>']\n",
    "\n",
    "def read_txt(path_to_file):\n",
    "    txt_ls = []\n",
    "    label_ls = []\n",
    "\n",
    "    with open(path_to_file) as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            id_num, txt, label = line.split('\\t')\n",
    "            txt_ls.append(txt)\n",
    "            label_ls.append(int(label.replace('\\n','')))\n",
    "    return txt_ls, label_ls\n",
    "\n",
    "\n",
    "def convert_word_to_idx(sents):\n",
    "    for sent in sents:\n",
    "        yield [w2i_dict[word] for word in sent.split(' ')]\n",
    "    return\n",
    "\n",
    "\n",
    "def add_padding(sents, max_len):\n",
    "    for i, sent in enumerate(sents):\n",
    "        if len(sent)< max_len:\n",
    "            sents[i] += [pad] * (max_len - len(sent))\n",
    "    \n",
    "        elif len(sent) > max_len:\n",
    "            sents[i] = sent[:max_len]\n",
    "    \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "train_txt_ls, train_label_ls = read_txt('ratings_train.txt')\n",
    "test_txt_ls, test_label_ls = read_txt('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 50000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_txt_ls), len(test_txt_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i_dict = defaultdict(lambda : len(w2i_dict))\n",
    "\n",
    "train_w2i_ls = list(convert_word_to_idx(train_txt_ls))\n",
    "test_w2i_ls = list(convert_word_to_idx(test_txt_ls))\n",
    "\n",
    "i2w_dict = {val : key for key, val in w2i_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아\n",
      "더빙..\n",
      "진짜\n",
      "짜증나네요\n",
      "목소리\n"
     ]
    }
   ],
   "source": [
    "for w2i in train_w2i_ls[0]:\n",
    "    print(i2w_dict[w2i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 모델 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_variable(w2i_ls):\n",
    "    \n",
    "    var = Variable(torch.LongTensor(w2i_ls))\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = convert_to_variable(add_padding(train_w2i_ls, 15))\n",
    "x_val = convert_to_variable(add_padding(test_w2i_ls[:10000],15))\n",
    "x_test = convert_to_variable(add_padding(test_w2i_ls[10000:],15))\n",
    "\n",
    "y_train = convert_to_variable(train_label_ls).float()\n",
    "y_val = convert_to_variable(test_label_ls[:10000]).float()\n",
    "y_test = convert_to_variable(test_label_ls[10000:]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_text(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_words, embed_size, hid_size, drop_rate, kernel_size_ls, num_filter):\n",
    "        super(CNN_text, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.hid_size = hid_size\n",
    "        self.drop_rate = drop_rate\n",
    "        self.num_filter = num_filter\n",
    "        self.kernel_size_ls = kernel_size_ls\n",
    "        self.num_kernel = len(kernel_size_ls)\n",
    "    \n",
    "        self.embedding = nn.Embedding(n_words, embed_size)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, num_filter, (kernel_size, embed_size)) for kernel_size in kernel_size_ls])\n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(self.num_kernel*num_filter, hid_size), nn.ReLU(), \n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(hid_size, 1),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x) # [batch_size, max_length, embed_size]\n",
    "        embed.unsqueeze_(1)  # [batch_size, 1, max_length, embed_size]\n",
    "        conved = [conv(embed).squeeze(3) for conv in self.convs] # [batch_size, num_filter, max_length -kernel_size +1]\n",
    "        pooled = [F.max_pool1d(conv, (conv.size(2))).squeeze(2) for conv in conved] # [batch_size, num_kernel, num_filter]\n",
    "        concated = torch.cat(pooled, dim = 1) # [batch_size, num_kernel * num_filter]\n",
    "        logit = self.lin(concated)\n",
    "        return torch.sigmoid(logit)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(w2i_dict)\n",
    "EMBED_SIZE = 64\n",
    "HID_SIZE = 64\n",
    "DROP_RATE = 0.5\n",
    "KERNEL_SIZE_LS = [2,3,4,5]\n",
    "NUM_FILTER = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_text(n_words = n_words, embed_size =EMBED_SIZE, drop_rate= DROP_RATE,\n",
    "                 hid_size=HID_SIZE, kernel_size_ls= KERNEL_SIZE_LS, num_filter=NUM_FILTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_text(\n",
       "  (embedding): Embedding(450542, 64)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 16, kernel_size=(2, 64), stride=(1, 1))\n",
       "    (1): Conv2d(1, 16, kernel_size=(3, 64), stride=(1, 1))\n",
       "    (2): Conv2d(1, 16, kernel_size=(4, 64), stride=(1, 1))\n",
       "    (3): Conv2d(1, 16, kernel_size=(5, 64), stride=(1, 1))\n",
       "  )\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch :1,  loss : 105025.5703125,  accuracy :0.502\n",
      "Test Loss : 6926.19580078125\n",
      "Test Accuracy : 0.515\n",
      "Train epoch :2,  loss : 104215.4375,  accuracy :0.513\n",
      "Test Loss : 6911.78466796875\n",
      "Test Accuracy : 0.526\n",
      "Train epoch :3,  loss : 103785.40625,  accuracy :0.527\n",
      "Test Loss : 6900.783203125\n",
      "Test Accuracy : 0.540\n",
      "Train epoch :4,  loss : 103407.75,  accuracy :0.545\n",
      "Test Loss : 6881.7939453125\n",
      "Test Accuracy : 0.547\n",
      "Train epoch :5,  loss : 102946.765625,  accuracy :0.560\n",
      "Test Loss : 6855.4140625\n",
      "Test Accuracy : 0.552\n",
      "Train epoch :6,  loss : 102368.8125,  accuracy :0.571\n",
      "Test Loss : 6819.2724609375\n",
      "Test Accuracy : 0.563\n",
      "Train epoch :7,  loss : 101599.40625,  accuracy :0.584\n",
      "Test Loss : 6773.35498046875\n",
      "Test Accuracy : 0.573\n",
      "Train epoch :8,  loss : 100493.96875,  accuracy :0.605\n",
      "Test Loss : 6715.81103515625\n",
      "Test Accuracy : 0.579\n",
      "Train epoch :9,  loss : 99160.6328125,  accuracy :0.605\n",
      "Test Loss : 6648.8291015625\n",
      "Test Accuracy : 0.589\n",
      "Train epoch :10,  loss : 97579.2890625,  accuracy :0.621\n",
      "Test Loss : 6585.58349609375\n",
      "Test Accuracy : 0.597\n",
      "Train epoch :11,  loss : 95979.5859375,  accuracy :0.639\n",
      "Test Loss : 6523.041015625\n",
      "Test Accuracy : 0.608\n",
      "Train epoch :12,  loss : 94258.5390625,  accuracy :0.655\n",
      "Test Loss : 6477.55029296875\n",
      "Test Accuracy : 0.609\n",
      "Train epoch :13,  loss : 92388.78125,  accuracy :0.663\n",
      "Test Loss : 6417.8369140625\n",
      "Test Accuracy : 0.622\n",
      "Train epoch :14,  loss : 90628.296875,  accuracy :0.681\n",
      "Test Loss : 6373.97509765625\n",
      "Test Accuracy : 0.625\n",
      "Train epoch :15,  loss : 88760.6875,  accuracy :0.685\n",
      "Test Loss : 6329.06494140625\n",
      "Test Accuracy : 0.632\n",
      "Train epoch :16,  loss : 86893.828125,  accuracy :0.706\n",
      "Test Loss : 6290.52392578125\n",
      "Test Accuracy : 0.639\n",
      "Train epoch :17,  loss : 85020.09375,  accuracy :0.715\n",
      "Test Loss : 6259.63037109375\n",
      "Test Accuracy : 0.642\n",
      "Train epoch :18,  loss : 83117.40625,  accuracy :0.721\n",
      "Test Loss : 6239.22607421875\n",
      "Test Accuracy : 0.649\n",
      "Train epoch :19,  loss : 81200.9921875,  accuracy :0.724\n",
      "Test Loss : 6218.39208984375\n",
      "Test Accuracy : 0.652\n",
      "Train epoch :20,  loss : 79461.5,  accuracy :0.740\n",
      "Test Loss : 6205.5654296875\n",
      "Test Accuracy : 0.657\n",
      "Train epoch :21,  loss : 77490.828125,  accuracy :0.748\n",
      "Test Loss : 6205.5029296875\n",
      "Test Accuracy : 0.660\n",
      "Train epoch :22,  loss : 75667.5390625,  accuracy :0.756\n",
      "Test Loss : 6193.87451171875\n",
      "Test Accuracy : 0.663\n",
      "Train epoch :23,  loss : 73843.59375,  accuracy :0.760\n",
      "Test Loss : 6203.138671875\n",
      "Test Accuracy : 0.666\n",
      "Train epoch :24,  loss : 72042.5625,  accuracy :0.768\n",
      "Test Loss : 6209.93603515625\n",
      "Test Accuracy : 0.668\n",
      "Train epoch :25,  loss : 70258.546875,  accuracy :0.777\n",
      "Test Loss : 6227.720703125\n",
      "Test Accuracy : 0.669\n",
      "Train epoch :26,  loss : 68447.5390625,  accuracy :0.778\n",
      "Test Loss : 6253.751953125\n",
      "Test Accuracy : 0.669\n",
      "Train epoch :27,  loss : 66843.15625,  accuracy :0.794\n",
      "Test Loss : 6257.30224609375\n",
      "Test Accuracy : 0.672\n",
      "Train epoch :28,  loss : 65017.5703125,  accuracy :0.807\n",
      "Test Loss : 6298.68310546875\n",
      "Test Accuracy : 0.674\n",
      "Train epoch :29,  loss : 63370.9375,  accuracy :0.802\n",
      "Test Loss : 6325.87353515625\n",
      "Test Accuracy : 0.677\n",
      "Train epoch :30,  loss : 61624.953125,  accuracy :0.817\n",
      "Test Loss : 6359.38427734375\n",
      "Test Accuracy : 0.678\n",
      "Train epoch :31,  loss : 59969.37890625,  accuracy :0.819\n",
      "Test Loss : 6393.74462890625\n",
      "Test Accuracy : 0.678\n",
      "Train epoch :32,  loss : 58403.33203125,  accuracy :0.828\n",
      "Test Loss : 6451.90380859375\n",
      "Test Accuracy : 0.680\n",
      "Train epoch :33,  loss : 56881.8828125,  accuracy :0.831\n",
      "Test Loss : 6481.22412109375\n",
      "Test Accuracy : 0.682\n",
      "Train epoch :34,  loss : 55281.35546875,  accuracy :0.839\n",
      "Test Loss : 6541.0927734375\n",
      "Test Accuracy : 0.683\n",
      "Train epoch :35,  loss : 53791.22265625,  accuracy :0.850\n",
      "Test Loss : 6584.408203125\n",
      "Test Accuracy : 0.684\n",
      "Train epoch :36,  loss : 52289.5390625,  accuracy :0.856\n",
      "Test Loss : 6638.1884765625\n",
      "Test Accuracy : 0.684\n",
      "Train epoch :37,  loss : 50681.5859375,  accuracy :0.855\n",
      "Test Loss : 6742.35107421875\n",
      "Test Accuracy : 0.683\n",
      "Train epoch :38,  loss : 49243.3203125,  accuracy :0.865\n",
      "Test Loss : 6799.49560546875\n",
      "Test Accuracy : 0.685\n",
      "Train epoch :39,  loss : 47937.01953125,  accuracy :0.870\n",
      "Test Loss : 6852.27587890625\n",
      "Test Accuracy : 0.685\n",
      "Train epoch :40,  loss : 46460.12890625,  accuracy :0.874\n",
      "Test Loss : 6904.146484375\n",
      "Test Accuracy : 0.684\n",
      "Train epoch :41,  loss : 45099.984375,  accuracy :0.877\n",
      "Test Loss : 6967.9189453125\n",
      "Test Accuracy : 0.685\n",
      "Train epoch :42,  loss : 43834.609375,  accuracy :0.883\n",
      "Test Loss : 7077.7919921875\n",
      "Test Accuracy : 0.684\n",
      "Train epoch :43,  loss : 42448.21875,  accuracy :0.881\n",
      "Test Loss : 7135.236328125\n",
      "Test Accuracy : 0.686\n",
      "Train epoch :44,  loss : 41173.5859375,  accuracy :0.889\n",
      "Test Loss : 7197.001953125\n",
      "Test Accuracy : 0.686\n",
      "Train epoch :45,  loss : 39787.26953125,  accuracy :0.891\n",
      "Test Loss : 7291.68017578125\n",
      "Test Accuracy : 0.687\n",
      "Train epoch :46,  loss : 38551.16015625,  accuracy :0.896\n",
      "Test Loss : 7371.66796875\n",
      "Test Accuracy : 0.687\n",
      "Train epoch :47,  loss : 37531.4765625,  accuracy :0.905\n",
      "Test Loss : 7459.29443359375\n",
      "Test Accuracy : 0.687\n",
      "Train epoch :48,  loss : 36445.82421875,  accuracy :0.907\n",
      "Test Loss : 7515.416015625\n",
      "Test Accuracy : 0.688\n",
      "Train epoch :49,  loss : 35168.88671875,  accuracy :0.910\n",
      "Test Loss : 7632.859375\n",
      "Test Accuracy : 0.685\n",
      "Train epoch :50,  loss : 34157.38671875,  accuracy :0.912\n",
      "Test Loss : 7738.7841796875\n",
      "Test Accuracy : 0.687\n",
      "Train epoch :51,  loss : 33024.8359375,  accuracy :0.915\n",
      "Test Loss : 7823.27197265625\n",
      "Test Accuracy : 0.689\n",
      "Train epoch :52,  loss : 32041.705078125,  accuracy :0.922\n",
      "Test Loss : 7892.57470703125\n",
      "Test Accuracy : 0.687\n",
      "Train epoch :53,  loss : 31024.77734375,  accuracy :0.923\n",
      "Test Loss : 7995.37890625\n",
      "Test Accuracy : 0.687\n",
      "Train epoch :54,  loss : 29984.451171875,  accuracy :0.925\n",
      "Test Loss : 8130.23828125\n",
      "Test Accuracy : 0.689\n",
      "Train epoch :55,  loss : 29146.673828125,  accuracy :0.928\n",
      "Test Loss : 8174.33935546875\n",
      "Test Accuracy : 0.690\n",
      "Train epoch :56,  loss : 28176.37890625,  accuracy :0.932\n",
      "Test Loss : 8300.2666015625\n",
      "Test Accuracy : 0.689\n",
      "Train epoch :57,  loss : 27398.376953125,  accuracy :0.937\n",
      "Test Loss : 8407.5595703125\n",
      "Test Accuracy : 0.689\n",
      "Train epoch :58,  loss : 26675.46875,  accuracy :0.940\n",
      "Test Loss : 8494.134765625\n",
      "Test Accuracy : 0.689\n",
      "Train epoch :59,  loss : 25482.771484375,  accuracy :0.942\n",
      "Test Loss : 8629.974609375\n",
      "Test Accuracy : 0.690\n",
      "Train epoch :60,  loss : 24676.615234375,  accuracy :0.937\n",
      "Test Loss : 8742.4248046875\n",
      "Test Accuracy : 0.690\n",
      "Train epoch :61,  loss : 23895.36328125,  accuracy :0.942\n",
      "Test Loss : 8840.0625\n",
      "Test Accuracy : 0.691\n",
      "Train epoch :62,  loss : 23117.2265625,  accuracy :0.944\n",
      "Test Loss : 8928.228515625\n",
      "Test Accuracy : 0.691\n",
      "Train epoch :63,  loss : 22442.158203125,  accuracy :0.949\n",
      "Test Loss : 9042.552734375\n",
      "Test Accuracy : 0.691\n",
      "Train epoch :64,  loss : 21830.578125,  accuracy :0.947\n",
      "Test Loss : 9185.4404296875\n",
      "Test Accuracy : 0.692\n",
      "Train epoch :65,  loss : 21052.201171875,  accuracy :0.952\n",
      "Test Loss : 9241.4072265625\n",
      "Test Accuracy : 0.690\n",
      "Train epoch :66,  loss : 20417.576171875,  accuracy :0.949\n",
      "Test Loss : 9382.55859375\n",
      "Test Accuracy : 0.692\n",
      "Train epoch :67,  loss : 19615.341796875,  accuracy :0.954\n",
      "Test Loss : 9531.4248046875\n",
      "Test Accuracy : 0.690\n",
      "Train epoch :68,  loss : 19052.91015625,  accuracy :0.956\n",
      "Test Loss : 9623.98828125\n",
      "Test Accuracy : 0.690\n",
      "Train epoch :69,  loss : 18532.77734375,  accuracy :0.958\n",
      "Test Loss : 9734.9521484375\n",
      "Test Accuracy : 0.691\n",
      "Train epoch :70,  loss : 17971.103515625,  accuracy :0.960\n",
      "Test Loss : 9813.7802734375\n",
      "Test Accuracy : 0.690\n",
      "Train epoch :71,  loss : 17302.115234375,  accuracy :0.964\n",
      "Test Loss : 10017.01171875\n",
      "Test Accuracy : 0.691\n",
      "Train epoch :72,  loss : 16753.95703125,  accuracy :0.963\n",
      "Test Loss : 10059.0537109375\n",
      "Test Accuracy : 0.689\n",
      "Train epoch :73,  loss : 16343.63671875,  accuracy :0.962\n",
      "Test Loss : 10170.7001953125\n",
      "Test Accuracy : 0.691\n",
      "Train epoch :74,  loss : 15650.671875,  accuracy :0.967\n",
      "Test Loss : 10281.3994140625\n",
      "Test Accuracy : 0.690\n",
      "Train epoch :75,  loss : 15274.2646484375,  accuracy :0.966\n",
      "Test Loss : 10414.0751953125\n",
      "Test Accuracy : 0.690\n",
      "Train epoch :76,  loss : 14733.41015625,  accuracy :0.971\n",
      "Test Loss : 10518.2861328125\n",
      "Test Accuracy : 0.688\n",
      "Train epoch :77,  loss : 14228.828125,  accuracy :0.972\n",
      "Test Loss : 10647.2275390625\n",
      "Test Accuracy : 0.689\n",
      "Train epoch :78,  loss : 13785.7685546875,  accuracy :0.971\n",
      "Test Loss : 10780.84765625\n",
      "Test Accuracy : 0.690\n",
      "Train epoch :79,  loss : 13494.595703125,  accuracy :0.970\n",
      "Test Loss : 10830.9365234375\n",
      "Test Accuracy : 0.690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-ed6d9065a215>\", line 33, in <module>\n",
      "    loss.backward()\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/torch/tensor.py\", line 102, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/torch/autograd/__init__.py\", line 90, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/inspect.py\", line 1459, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/inspect.py\", line 1417, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/inspect.py\", line 677, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/inspect.py\", line 714, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 0.0003\n",
    "batch_size = 10000\n",
    "\n",
    "train_idx = np.arange(x_train.size(0))\n",
    "test_idx = np.arange(x_test.size(0))\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr)\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # input 데이터 순서 섞기\n",
    "    random.shuffle(train_idx)\n",
    "    x_train = x_train[train_idx]\n",
    "    y_train = y_train[train_idx]\n",
    "    train_loss = 0\n",
    "\n",
    "    for start_idx, end_idx in zip(range(0, x_train.size(0), batch_size),\n",
    "                                  range(batch_size, x_train.size(0)+1, batch_size)):\n",
    "        x_batch = x_train[start_idx : end_idx]\n",
    "        y_batch = y_train[start_idx : end_idx]\n",
    "        \n",
    "        logit = model(x_batch)\n",
    "        predict = logit.ge(0.5).float()\n",
    "        y_batch = y_batch.unsqueeze(1)\n",
    "        \n",
    "        acc = (predict == y_batch).sum().item() / batch_size\n",
    "        loss = criterion(logit, y_batch)\n",
    "        train_loss += loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Train epoch : %s,  loss : %s,  accuracy :%.3f'%(epoch+1, train_loss.item(), acc))\n",
    "    print('=================================================================================================')\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        model.eval()\n",
    "        logit = model(x_val).squeeze(1)\n",
    "        predict = logit.ge(0.5).float()\n",
    "        \n",
    "        acc = (predict == y_val).sum().item() / 10000\n",
    "        loss = criterion(logit, y_val)\n",
    "        \n",
    "        print('Test Epoch : %s, Test Loss : %.03f , Test Accuracy : %.03f'%(epoch+1, loss.item(), acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
