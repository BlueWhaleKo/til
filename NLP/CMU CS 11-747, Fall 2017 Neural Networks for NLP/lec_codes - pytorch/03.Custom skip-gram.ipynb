{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'warsaw is poland capital',\n",
    "    'berlin is germany capital',\n",
    "    'paris is france capital',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [sent.split() for sent in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2idx, idx2word 사전 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = defaultdict(lambda : len(word2idx))\n",
    "\n",
    "# word2idx dict\n",
    "for sent in tokenized_corpus:\n",
    "    for token in sent:\n",
    "        word2idx[token]\n",
    "        \n",
    "# idx2word\n",
    "idx2word = {}\n",
    "for word, idx in word2idx.items():\n",
    "    idx2word[idx] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "n_words = len(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip gram의 context 형태 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_pair_corpus = []\n",
    "\n",
    "for token_ls in tokenized_corpus:\n",
    "    idx_ls = [word2idx[word] for word in token_ls]\n",
    "\n",
    "    temp_tokenized_idx_ls = []\n",
    "    for i, idx in enumerate(idx_ls):\n",
    "        for w in range(-window_size, window_size+1):\n",
    "            if i+w < 0 or i+w >= len(idx_ls) or w == 0: continue\n",
    "            else :\n",
    "                temp_tokenized_idx_ls.append([idx_ls[i], idx_ls[i+w]])\n",
    "    \n",
    "    idx_pair_corpus.append(temp_tokenized_idx_ls)\n",
    "\n",
    "idx_pair_corpus = np.array(idx_pair_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 2],\n",
       "       [1, 0],\n",
       "       [1, 2],\n",
       "       [1, 3],\n",
       "       [2, 0],\n",
       "       [2, 1],\n",
       "       [2, 3],\n",
       "       [3, 1],\n",
       "       [3, 2]])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_pair_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(word_idx):\n",
    "    x = torch.zeros(n_words).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 5\n",
    "\n",
    "w1 = Variable(torch.randn((embedding_size, n_words)).float(), requires_grad = True)\n",
    "w2 = Variable(torch.randn((n_words, embedding_size)).float(), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epo :0, tensor(10.9632)\n",
      "Loss at epo :100, tensor(10.8446)\n",
      "Loss at epo :200, tensor(10.8273)\n",
      "Loss at epo :300, tensor(10.8148)\n",
      "Loss at epo :400, tensor(10.8045)\n",
      "Loss at epo :500, tensor(10.7957)\n",
      "Loss at epo :600, tensor(10.7881)\n",
      "Loss at epo :700, tensor(10.7815)\n",
      "Loss at epo :800, tensor(10.7756)\n",
      "Loss at epo :900, tensor(10.7704)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(1000):\n",
    "    loss_val = 0\n",
    "    for idx_pairs in idx_pair_corpus:\n",
    "        for center_word_idx, target_word_idx in idx_pairs:\n",
    "            x = Variable(one_hot_encode(center_word_idx), requires_grad = False)\n",
    "            y_true = Variable(torch.tensor([target_word_idx]))\n",
    "\n",
    "            z1 = torch.matmul(w1, x)\n",
    "            z2 = torch.matmul(w2, z1)\n",
    "\n",
    "            log_softmax = F.log_softmax(z2, dim = 0)\n",
    "\n",
    "            loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "\n",
    "            loss_val += loss.data\n",
    "            loss.backward()\n",
    "\n",
    "            w1.data -= learning_rate * w1.grad.data\n",
    "            w2.data -= learning_rate * w2.grad.data\n",
    "\n",
    "            w1.grad.data.zero_()\n",
    "            w2.grad.data.zero_()\n",
    "\n",
    "    if epoch % 100 == 0:    \n",
    "        print('Loss at epo :%s, %s'%(epoch, loss_val/len(idx_pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = idx_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0969, -1.3024, -1.3910,  0.0915,  0.2805])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(embedding_matrix.permute(1,0), get_input_layer(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
